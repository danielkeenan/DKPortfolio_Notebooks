{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Category:\n",
    "    BOOKS = \"BOOKS\"\n",
    "    CLOTHING = \"CLOTHING\"\n",
    "\n",
    "train_x = [\"i love the book\",\"this is a great book\", \"the fit is great\", \"i love the shoes\"]\n",
    "train_y = [Category.BOOKS, Category.BOOKS, Category.CLOTHING, Category.CLOTHING]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 0)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "train_x_vectors = vectorizer.fit_transform(train_x)\n",
    "\n",
    "print(vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['book', 'fit', 'great', 'is', 'love', 'shoes', 'the', 'this']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 1, 0, 1, 0],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 1],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 1, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf_svm = svm.SVC(kernel='linear')\n",
    "clf_svm.fit(train_x_vectors, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BOOKS'], dtype='<U8')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = vectorizer.transform(['i enjoy this book'])\n",
    "\n",
    "clf_svm.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i love the book', 'this is a great book', 'the fit is great', 'i love the shoes']\n"
     ]
    }
   ],
   "source": [
    "print(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [nlp(text) for text in train_x]\n",
    "train_x_word_vectors = [x.vector for x in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.08563001  0.313255   -0.2392405  -0.17215225  0.1418515   0.1970548\n",
      "  0.04868999 -0.12744625  0.05947001  2.1347     -0.61964     0.01162549\n",
      "  0.29980502 -0.125354    0.017935   -0.1355105  -0.27094752  1.1129825\n",
      " -0.16986902 -0.0266875   0.14768225 -0.16372526  0.121907   -0.06876825\n",
      " -0.061945    0.08704174 -0.2005705  -0.24039775 -0.0675595   0.0926495\n",
      " -0.13526568  0.24121101 -0.20299     0.30007     0.11574501  0.055062\n",
      "  0.013516   -0.0664179  -0.3380587  -0.17823698 -0.01039225  0.03333575\n",
      " -0.10241525 -0.093445    0.09327275  0.20661727 -0.15074751  0.14018372\n",
      "  0.23520125 -0.05192125 -0.0999365  -0.1212635  -0.05895525 -0.005062\n",
      "  0.06003174  0.01213001 -0.11257375 -0.24570274  0.00678    -0.1888345\n",
      " -0.09276348 -0.25614128 -0.20717824  0.0858725  -0.02215025 -0.303222\n",
      " -0.00274375  0.11888     0.02695867  0.20738849  0.02140525 -0.0175935\n",
      "  0.1513575  -0.0032025   0.20425075  0.16609626 -0.084585   -0.0744465\n",
      " -0.1083965   0.14420825  0.13595775  0.2158625   0.15477975 -0.04820438\n",
      "  0.23010999 -0.2999731  -0.126152   -0.42502826  0.17733926  0.08174075\n",
      " -0.26656875 -0.101404   -0.253544    0.05471925  0.2847125   0.09851776\n",
      "  0.204341    0.16268425 -0.042765   -0.1949425   0.10958    -0.3648075\n",
      " -0.09682125 -0.11094925  0.12445608 -0.359581    0.15130275  0.06853426\n",
      " -0.109819   -0.21615925  0.0958285  -0.00383826  0.26055914 -0.35583502\n",
      "  0.0842915  -0.14935926 -0.1996505   0.23104301 -0.0077375  -0.06271806\n",
      "  0.17203225 -0.181445    0.03677999  0.09908225  0.2382695   0.4065215\n",
      " -0.0908625  -0.15399225  0.08435975 -0.053317   -0.20035249 -0.163792\n",
      " -0.271757    0.01030713  0.03340515  0.0497255  -0.13881615  0.084502\n",
      " -0.053755   -0.31270498 -2.257525   -0.1419485   0.14178     0.08852901\n",
      " -0.17659001 -0.13609049 -0.04871     0.09790501 -0.08347125 -0.20770678\n",
      " -0.04589     0.13999274  0.16002081  0.0629995   0.0256235  -0.04082\n",
      " -0.15203801 -0.1771985  -0.217835   -0.039533    0.089236    0.18021\n",
      "  0.015082    0.03417152 -0.07016499 -0.13753426  0.035425   -0.19859773\n",
      "  0.1456615   0.09975475 -0.100101   -0.0668525   0.22425    -0.452075\n",
      "  0.11908074  0.05990475 -0.0488925   0.1621385  -0.101395   -0.1326985\n",
      "  0.069592   -0.16106975 -0.1380995  -0.22239     0.05668125 -0.0171395\n",
      " -0.29649    -0.3353225   0.06168649  0.09971251 -0.15555725 -0.09166402\n",
      " -0.282595   -0.16052501 -0.02805945  0.15195748  0.07515249 -0.27176827\n",
      "  0.05228776  0.00373    -0.130024   -0.22874999 -0.07862875 -0.0399495\n",
      "  0.264615    0.098764    0.0206625   0.05326832 -0.1118725   0.10922225\n",
      "  0.00446974  0.115469   -0.17844    -0.38571697  0.0452885   0.24619749\n",
      " -0.008345   -0.2139215  -0.0598165   0.1438475   0.1524835  -0.08872448\n",
      "  0.07794543 -0.01462124 -0.273732   -0.07258825  0.1016425   0.16666101\n",
      "  0.0239075   0.03709501 -0.267228    0.0838585  -0.10917226  0.07474675\n",
      " -0.2799775  -0.23018251 -0.15760949 -0.02298325 -0.16043949  0.22297475\n",
      "  0.04065251  0.11423799 -0.095754    0.207424    0.12350225 -0.21028924\n",
      " -0.09836699  0.024505   -0.1130115   0.01491502  0.18254225 -0.309755\n",
      " -0.13121249 -0.008149    0.17937374  0.1328375   0.13090524 -0.09056126\n",
      " -0.17138    -0.12233325  0.12965076  0.020424    0.05174625  0.22267\n",
      "  0.26246977 -0.215415    0.13679275  0.2902495   0.25620502  0.0093\n",
      " -0.22721347 -0.26039124 -0.36055002  0.01478199  0.0270805   0.03114517\n",
      " -0.24702299 -0.02437525 -0.03598415  0.22466755  0.0266175  -0.08971775\n",
      " -0.14917499 -0.30266726 -0.05662975  0.0756845   0.35060728  0.1437245\n",
      " -0.0064325  -0.15310623 -0.1476075   0.09711101 -0.09789225 -0.06524446\n",
      "  0.32439575 -0.06215625  0.20532525 -0.040052    0.0630795   0.14291999]\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svm_wv = svm.SVC(kernel='linear')\n",
    "clf_svm_wv.fit(train_x_word_vectors, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BOOKS'], dtype='<U8')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = [\"i love the book\"]\n",
    "test_docs = [nlp(text) for text in test_x]\n",
    "test_x_word_vectors = [x.vector for x in test_docs]\n",
    "clf_svm_wv.predict(test_x_word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DEJK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DEJK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DEJK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reading', 'the', 'books']\n",
      "['read', 'the', 'book']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "phrase = \"reading the books\"\n",
    "words = word_tokenize(phrase)\n",
    "print(words)\n",
    "\n",
    "stemmed_words = []\n",
    "for word in words: \n",
    "    stemmed_words.append(stemmer.stem(word))\n",
    "    \n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reading', 'the', 'books']\n",
      "['reading', 'the', 'book']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "phrase = \"reading the books\"\n",
    "words = word_tokenize(phrase)\n",
    "print(words)\n",
    "\n",
    "lemmatized_words = []\n",
    "for word in words:\n",
    "    lemmatized_words.append(lemmatizer.lemmatize(word))\n",
    "    \n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Here', 'example', 'sentence', 'demonstrating', 'removal', 'stopwords']\n"
     ]
    }
   ],
   "source": [
    "phrase = \"Here is an example sentence demonstrating the removal of stopwords\"\n",
    "\n",
    "words = word_tokenize(phrase)\n",
    "\n",
    "stripped_phrase = []\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        stripped_phrase.append(word)\n",
    "\n",
    "print(stripped_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.8, subjectivity=0.75)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "phrase = \"the book was great\"\n",
    "\n",
    "tb_phrase = TextBlob(phrase)\n",
    "tb_phrase = tb_phrase.correct()\n",
    "tb_phrase.tags\n",
    "\n",
    "tb_phrase.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
